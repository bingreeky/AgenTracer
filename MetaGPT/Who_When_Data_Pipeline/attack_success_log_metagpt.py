#!/usr/bin/env python3
"""
Attack Success Log Processing Module - Using MetaGPT framework for attack analysis
"""

import os
import sys
import json
import asyncio
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime

# Add parent directory to path
sys.path.append(os.path.dirname(os.path.dirname(__file__)))

from attack_monitor import AdvancedAttackMonitor, PromptInjectionInterceptor


def standardize_role_name(name: str) -> str:
    """Standardize role names, ensure using correct role names instead of personal names"""
    # Role name mapping
    role_mapping = {
        # Personal name to role name mapping
        "Mike": "Team Leader",
        "Alice": "Product Manager",
        "Bob": "Architect",
        "Alex": "Engineer",
        "David": "Data Analyst",
        # Keep existing role names unchanged
        "Team Leader": "Team Leader",
        "Product Manager": "Product Manager",
        "Architect": "Architect",
        "Engineer": "Engineer",
        "Data Analyst": "Data Analyst",
        # Handle possible variants
        "TeamLeader": "Team Leader",
        "ProductManager": "Product Manager",
        "DataAnalyst": "Data Analyst",
        "Engineer2": "Engineer",
    }

    return role_mapping.get(name, name)


# Use environment variables or relative paths
metagpt_path = os.getenv(
    "METAGPT_PATH", os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
)
sys.path.append(metagpt_path)
from metagpt.software_company import generate_repo

# Import advanced monitor
from Who_When_Data_Pipeline.attack_monitor import (
    AdvancedAttackMonitor,
    PromptInjectionInterceptor,
)

# Directory configuration - Use environment variables or default paths
base_dir = os.path.dirname(os.path.abspath(__file__))

# Detect dataset type
dataset_type = "mbpp+"  # Default value
if "kodcode" in base_dir:
    dataset_type = "kodcode"
elif "mbpp" in base_dir:
    dataset_type = "mbpp+"

# Set default paths based on dataset type
if dataset_type == "kodcode":
    default_attacked_log_base = os.path.join(base_dir, "kodcode/attacked_logs_round_3")
    default_attack_suggestion_log_base = os.path.join(
        base_dir, "kodcode/attack_suggestions_round_3"
    )
    default_attack_record_dir = os.path.join(base_dir, "kodcode/attack_records_round_3")
    default_success_log_dir = os.path.join(
        base_dir, "kodcode/attacked_still_succeed_tasks_round_2.json"
    )
else:
    default_attacked_log_base = os.path.join(base_dir, "mbpp+/attacked_logs_round_3")
    default_attack_suggestion_log_base = os.path.join(
        base_dir, "mbpp+/attack_suggestions_round_3"
    )
    default_attack_record_dir = os.path.join(base_dir, "mbpp+/attack_records_round_3")
    default_success_log_dir = os.path.join(
        base_dir, "mbpp+/attacked_still_succeed_tasks_round_2.json"
    )

ATTACKED_LOG_BASE = os.getenv("ATTACKED_LOG_BASE", default_attacked_log_base)
ATTACK_SUGGESTION_LOG_BASE = os.getenv(
    "ATTACK_SUGGESTION_LOG_BASE", default_attack_suggestion_log_base
)
ATTACK_RECORD_DIR = os.getenv("ATTACK_RECORD_DIR", default_attack_record_dir)
WORKSPACE_DIR = os.getenv(
    "WORKSPACE_DIR", os.path.join(os.path.dirname(base_dir), "workspace")
)

# Create necessary directories
os.makedirs(ATTACKED_LOG_BASE, exist_ok=True)
os.makedirs(ATTACK_SUGGESTION_LOG_BASE, exist_ok=True)
os.makedirs(WORKSPACE_DIR, exist_ok=True)
os.makedirs(ATTACK_RECORD_DIR, exist_ok=True)


def get_attacked_log_path(log_name):
    """Dynamically get attack log path"""
    attacked_log_base = os.getenv("ATTACKED_LOG_BASE", ATTACKED_LOG_BASE)
    return os.path.join(attacked_log_base, log_name)


def get_attack_suggestion_log_path(log_name):
    """Dynamically get attack suggestion log path"""
    attack_suggestion_log_base = os.getenv(
        "ATTACK_SUGGESTION_LOG_BASE", ATTACK_SUGGESTION_LOG_BASE
    )
    return os.path.join(attack_suggestion_log_base, log_name)


def get_attack_record_path(log_name):
    """Dynamically get attack record path"""
    attack_record_dir = os.getenv("ATTACK_RECORD_DIR", ATTACK_RECORD_DIR)
    return os.path.join(attack_record_dir, log_name)


def load_previous_attack_analysis(task_id: str) -> List[dict]:
    """Load all round attack analysis history - Backward compatibility function"""
    analyses = []

    # Dynamically get current round and path
    current_round = 3  # Default value
    attack_suggestion_log_base = os.getenv(
        "ATTACK_SUGGESTION_LOG_BASE", ATTACK_SUGGESTION_LOG_BASE
    )

    if "round_2" in attack_suggestion_log_base:
        current_round = 2
    elif "round_1" in attack_suggestion_log_base:
        current_round = 1
    else:
        # Try to extract round from path
        import re

        match = re.search(r"round_(\d+)", attack_suggestion_log_base)
        if match:
            current_round = int(match.group(1))

    print(f"[HISTORY] Current round: {current_round}")

    # Iterate through all previous rounds
    for round_num in range(1, current_round):
        # Build path for this round
        round_path = attack_suggestion_log_base.replace(
            f"round_{current_round}", f"round_{round_num}"
        )
        previous_path = os.path.join(round_path, f"{task_id.replace('/', '_')}.json")

        print(f"[HISTORY] Trying to load Round {round_num} attack analysis: {previous_path}")

        if os.path.exists(previous_path):
            try:
                with open(previous_path, "r", encoding="utf-8") as f:
                    data = json.load(f)
                    attack_analysis = data.get("attack_analysis")
                    if attack_analysis:
                        # Add round information
                        attack_analysis["round"] = round_num
                        analyses.append(attack_analysis)
                        print(
                            f"[HISTORY] Successfully loaded Round {round_num} attack analysis: step {attack_analysis.get('attack_step', 'Unknown')}"
                        )
                    else:
                        print(
                            f"[HISTORY] Round {round_num} file exists but missing attack_analysis field"
                        )
            except Exception as e:
                print(
                    f"[HISTORY] Error loading round {round_num} attack analysis from {previous_path}: {e}"
                )
        else:
            print(f"[HISTORY] Round {round_num} attack analysis file does not exist: {previous_path}")

    print(f"[HISTORY] Total loaded {len(analyses)} round historical attack analysis")
    return analyses


async def attack_with_meta_company(
    success_log, retry_count=0, previous_attack_analyses=None
):
    """Use MetaGPT's software company for attack analysis - Supports retry and historical attack analysis"""
    history = []
    step_counter = {"step": 0}

    def log_step(content, role, name):
        print(f"[LOG_STEP] {role} {name}: {content}")
        history.append(
            {
                "step": step_counter["step"],
                "content": content,
                "role": role,
                "name": name,
            }
        )
        step_counter["step"] += 1

    # Build attack prompt
    task_id = success_log["question_ID"]

    # Format original task history
    original_history = success_log.get("history", [])
    history_str = ""
    for step in original_history:
        history_str += f"\nStep {step['step']} - {step['name']} ({step['role']}):\n{step['content']}\n"

    # Build historical attack analysis information
    previous_attack_info = ""

    # Prioritize historical analysis information passed by framework
    if previous_attack_analyses is None and "previous_attack_analyses" in success_log:
        previous_attack_analyses = success_log["previous_attack_analyses"]
        print(
            f"[FRAMEWORK_HISTORY] Using historical attack analysis information passed by framework: {len(previous_attack_analyses)} rounds"
        )

    # If no historical information passed by framework, use original load method (backward compatibility)
    if previous_attack_analyses is None:
        print(f"[FALLBACK_HISTORY] Framework did not pass historical information, using original load method")
        previous_attack_analyses = load_previous_attack_analysis(task_id)

    if previous_attack_analyses and len(previous_attack_analyses) > 0:
        print("------previous attack analyses info added------\n")
        previous_attack_info = "IMPORTANT - Previous Attack Analysis History:\n"
        previous_attack_info += f"Multiple attack analyses have been attempted for this task ({len(previous_attack_analyses)} rounds):\n\n"

        for i, prev_attack in enumerate(previous_attack_analyses):
            round_num = prev_attack.get("round", i + 1)
            previous_attack_info += f"Round {round_num}:\n"
            previous_attack_info += (
                f"- Previous attack_step: {prev_attack.get('attack_step', 'Unknown')}\n"
            )
            previous_attack_info += f"- Previous mistake_agent: {prev_attack.get('mistake_agent', 'Unknown')}\n"
            previous_attack_info += f"- Previous original_content: {prev_attack.get('original_content', 'No content provided')}\n"
            previous_attack_info += f"- Previous attack_content: {prev_attack.get('attack_content', 'No attack content provided')}\n"
            previous_attack_info += f"- Previous reason: {prev_attack.get('reason', 'No reason provided')}\n\n"

        # Build excluded steps list
        excluded_steps = [
            str(prev_attack.get("attack_step", ""))
            for prev_attack in previous_attack_analyses
            if prev_attack.get("attack_step")
        ]
        excluded_steps_str = ", ".join(excluded_steps) if excluded_steps else "none"

        previous_attack_info += f"CRITICAL GUIDELINES FOR THIS ATTACK ANALYSIS:\n"
        previous_attack_info += f"1. DO NOT analyze any of the following steps again - they have already been identified and attempted: {excluded_steps_str}\n"
        previous_attack_info += f"2. Look for OTHER steps in the execution history that might contain the actual critical vulnerability.\n"
        previous_attack_info += f"3. Focus on analyzing different aspects of the implementation or different steps in the process.\n"
        previous_attack_info += f"4. Consider if the vulnerability might be in the task understanding, algorithm design, or a different implementation step.\n"
        previous_attack_info += f"5. Analyze the pattern of previous attempts to avoid similar ineffective approaches.\n"

    # Build historical constraint information
    history_constraint = ""
    if previous_attack_analyses and len(previous_attack_analyses) > 0:
        history_constraint = f"""
ğŸš¨ CRITICAL CONSTRAINT - YOU MUST FOLLOW THIS RULE:
You are FORBIDDEN from analyzing step(s): {excluded_steps_str}
If you analyze any of these steps again, your analysis will be REJECTED and considered a FAILURE.
You MUST choose a DIFFERENT step that has NOT been analyzed before.

MANDATORY ANALYSIS RULES:
1. You MUST NOT analyze step(s): {excluded_steps_str}
2. You MUST choose a DIFFERENT step that has NOT been analyzed in previous rounds
3. If you repeat any previous analysis, your response will be considered INVALID
4. Focus on steps that have NOT been identified as vulnerable in previous attempts
5. Consider alternative attack vectors and different implementation aspects
6. Look for vulnerabilities in different parts of the code or different agents' contributions

PREVIOUS ATTEMPTS SUMMARY:
"""
        for i, analysis in enumerate(previous_attack_analyses):
            history_constraint += f"- Round {analysis.get('round', i+1)}: Step {analysis.get('attack_step', 'Unknown')} ({analysis.get('mistake_agent', 'Unknown')})\n"

        history_constraint += f"""
YOUR TASK: Analyze a DIFFERENT step than those listed above. Choose step numbers other than: {excluded_steps_str}

VALIDATION CHECK: Before submitting your analysis, verify that your attack_step is NOT in this list: {excluded_steps_str}
If it is, you MUST choose a different step.
"""

    prompt = f"""You are an intelligent attack expert analyzing a successful MetaGPT execution for a programming task.

Your goal is to identify the most critical step in the execution history that, if modified, would lead to an incorrect final result.

Task Information:
Task ID: {task_id}
Question: {success_log['question']}
Ground Truth: {success_log['ground_truth']}
Model Prediction: {success_log['model_prediction']}

{previous_attack_info}

Original Task Execution History:
{history_str}

{history_constraint}

Your attack analysis should be in the following JSON format:
{{
    "attack_step": <step_number>,  // The step number where the attack should be applied
    "mistake_agent": "<the agent that made the mistake>",  // The agent that made the mistake
    "original_content": "<brief description of what this step does>",  // Brief description of the original step
    "attack_content": "<SPECIFIC, IMPLEMENTABLE attack instructions with concrete code changes>",  // SPECIFIC attack instructions
    "reason": "<detailed explanation of why this step is critical and how the attack would work>"  // Detailed explanation
}}

CRITICAL REQUIREMENTS FOR ATTACK_CONTENT:
1. The attack_content MUST be SPECIFIC and IMPLEMENTABLE
2. Include CONCRETE CODE CHANGES or MODIFICATIONS
3. Provide SPECIFIC instructions that can be directly executed
4. Focus on FUNCTIONALITY CHANGES that will cause the program to fail
5. Examples of good attack_content:
   - "Change the return statement from 'return tup + (dct,)' to 'return list(tup) + [dct]'"
   - "Modify the function to return None instead of the tuple"
   - "Add a bug: change 'return tup + (dct,)' to 'return tup + (dct, dct)' (duplicate the dictionary)"
   - "Change the function to ignore the dictionary: 'return tup'"
6. AVOID vague instructions like "return incorrect type" or "modify the function"

Important Guidelines:
1. Focus on identifying the root cause of potential failure, not just any step.
2. The 'attack_step' should be a number corresponding to a step in the implementation process.
3. The 'mistake_agent' should be the agent that made the mistake.
4. The 'original_content' should briefly describe what the step does.
5. The 'attack_content' MUST be SPECIFIC and IMPLEMENTABLE with concrete changes.
6. The 'reason' should be detailed and explain why this step is critical and how the attack would work.
7. Analyze the original task execution history to understand the context and identify where things could go wrong.
8. Focus on steps that involve code generation, implementation, or key algorithmic decisions.

CRITICAL REQUIREMENTS:
1. You MUST create the file FIRST using Editor.create_file
2. You MUST write the content SECOND using Editor.write
3. You MUST use the exact filename: "{task_id.replace('/', '_')}_attack_analysis.json"
4. You MUST NOT use the 'end' command until both file operations are completed
5. You MUST provide the attack analysis in valid JSON format

Step-by-step process:
1. First, create the file:
```json
[
    {{
        "command_name": "Editor.create_file",
        "args": {{
            "filename": "{task_id.replace('/', '_')}_attack_analysis.json"
        }}
    }}
]
```

2. Then, write the attack analysis content:
```json
[
    {{
        "command_name": "Editor.write",
        "args": {{
            "path": "{task_id.replace('/', '_')}_attack_analysis.json",
            "content": "{{"attack_step": "...", "original_content": "...", "attack_content": "SPECIFIC CODE CHANGES HERE", "reason": "..."}}"
        }}
    }}
]
```

3. Only after both file operations are successful, use the end command:
```json
[
    {{
        "command_name": "end"
    }}
]
```

Please analyze the task and provide your attack analysis."""

    try:
        # If it's a retry, add retry information to the prompt
        if retry_count > 0:
            prompt = f"ATTEMPT {retry_count + 1} (RETRY): " + prompt
            print(f"[ATTACK_ANALYSIS] Retry attempt {retry_count + 1} for {task_id}")

        team = await generate_repo(
            idea=prompt,
            n_round=5,
            code_review=True,
            run_tests=False,
            implement=False,
            project_name=f"attack_{task_id.replace('/', '_')}",
            project_path=WORKSPACE_DIR,
            project_dir=WORKSPACE_DIR,
            log_step=log_step,
            use_async=True,
        )

        # æ£€æŸ¥æ”»å‡»åˆ†æResult fileæ˜¯å¦ç”Ÿæˆ
        possible_paths = [
            os.path.join(
                WORKSPACE_DIR, f"{task_id.replace('/', '_')}_attack_analysis.json"
            ),
            os.path.join(
                WORKSPACE_DIR,
                f"attack_{task_id.replace('/', '_')}",
                f"{task_id.replace('/', '_')}_attack_analysis.json",
            ),
            os.path.join(
                WORKSPACE_DIR,
                f"attack_{task_id.replace('/', '_')}",
                "workspace",
                f"{task_id.replace('/', '_')}_attack_analysis.json",
            ),
        ]

        attack_file = None
        for path in possible_paths:
            if os.path.exists(path):
                attack_file = path
                break

        if attack_file:
            with open(attack_file, "r") as f:
                attack_analysis = json.load(f)
                return attack_analysis, history
        else:
            print(f"Warning: Attack analysis file not found for {task_id}")
            print(f"Searched in paths: {possible_paths}")

            # å°è¯•LoadedHistoricalè®°å½•ä¸­æå–æ”»å‡»åˆ†æ
            attack_analysis = extract_attack_analysis_from_history(history)
            if attack_analysis:
                print(f"Extracted attack analysis from history for {task_id}")
                return attack_analysis, history

            return None, None

    except Exception as e:
        print(f"Attack analysis failed: {e}")
        return None, None


def extract_attack_analysis_from_history(history):
    """LoadedHistoricalè®°å½•ä¸­æå–æ”»å‡»åˆ†æ"""
    for step in reversed(history):
        content = step.get("content", "")

        # æŸ¥æ‰¾JSONæ ¼å¼çš„æ”»å‡»åˆ†æ
        if '"attack_step"' in content and '"attack_content"' in content:
            try:
                # æå–JSONéƒ¨åˆ†
                start_idx = content.find("{")
                end_idx = content.rfind("}") + 1
                if start_idx != -1 and end_idx != -1:
                    json_str = content[start_idx:end_idx]
                    attack_analysis = json.loads(json_str)

                    # éªŒè¯å¿…è¦çš„å­—æ®µ
                    required_fields = [
                        "attack_step",
                        "original_content",
                        "attack_content",
                        "reason",
                    ]
                    if all(field in attack_analysis for field in required_fields):
                        return attack_analysis
            except json.JSONDecodeError:
                continue

    return None


async def generate_attacked_solution_with_advanced_monitoring(
    success_log, attack_analysis
):
    """ä½¿ç”¨é«˜çº§ç›‘æ§å™¨ç”Ÿæˆæ”»å‡»åçš„è§£å†³æ–¹æ¡ˆ"""
    history = []

    # åˆ›å»ºç»Ÿä¸€çš„æ­¥éª¤è®¡æ•°å™¨ - ä½¿ç”¨ç±»æ¥ç®¡ç†çŠ¶æ€
    class StepCounter:
        def __init__(self):
            self.step = 0
            self.roles = {}  # å­˜å‚¨æ‰€æœ‰Roleå®ä¾‹çš„å¼•ç”¨

        def increment(self):
            self.step += 1
            # åŒæ­¥æ‰€æœ‰Roleå®ä¾‹çš„æ­¥éª¤è®¡æ•°å™¨
            for role in self.roles.values():
                if hasattr(role, "step_counter"):
                    role.step_counter = self.step

        def get_step(self):
            return self.step

        def reset(self):
            """é‡ç½®æ­¥éª¤è®¡æ•°å™¨"""
            self.step = 0
            # åŒæ­¥æ‰€æœ‰Roleå®ä¾‹çš„æ­¥éª¤è®¡æ•°å™¨
            for role in self.roles.values():
                if hasattr(role, "step_counter"):
                    role.step_counter = 0

    step_counter = StepCounter()

    # é‡ç½®æ­¥éª¤è®¡æ•°å™¨ï¼Œç¡®ä¿æ¯æ¬¡æ–°çš„Taskæ‰§è¡Œéƒ½Loaded0å¼€å§‹
    step_counter.reset()

    # æ·»åŠ è°ƒè¯•ä¿¡æ¯
    print(f"[DEBUG] æ­¥éª¤è®¡æ•°å™¨é‡ç½®: {step_counter.get_step()}")

    # åˆ›å»ºé«˜çº§æ”»å‡»ç›‘æ§å™¨
    monitor = AdvancedAttackMonitor(
        success_log["question_ID"], attack_analysis, original_success_log=success_log
    )
    interceptor = PromptInjectionInterceptor(monitor)

    def log_step(content, role, name):
        print(f"[LOG_STEP] {role} {name}: {content}")

        # æ ‡å‡†åŒ–èŒè´£åç§°
        standardized_name = standardize_role_name(name)

        # æ£€æŸ¥æ˜¯å¦æ˜¯terminal outputï¼Œå¦‚æœæ˜¯åˆ™åˆå¹¶åˆ°ä¸Šä¸€ä¸ªstep
        if role == "Terminal" and content.startswith("Terminal output:"):
            # å°†terminal outputåˆå¹¶åˆ°ä¸Šä¸€ä¸ªstep
            if history:
                last_step = history[-1]
                # incontentä¸­æ·»åŠ terminal output
                last_step["content"] += f"\n\n{content}"
                print(
                    f"[LOG_STEP] Terminal output merged into previous step: {last_step['step']}"
                )
            else:
                # å¦‚æœæ²¡æœ‰ä¸Šä¸€ä¸ªstepï¼Œåˆ›å»ºä¸€ä¸ªæ–°çš„step
                current_step = step_counter.get_step()
                history.append(
                    {
                        "step": current_step,
                        "content": content,
                        "role": role,
                        "name": standardized_name,
                    }
                )
                step_counter.increment()
            return

        # è®°å½•ç›‘æ§äº‹ä»¶
        monitor.log_event("step", role, standardized_name, content)

        # æ£€æŸ¥æ˜¯å¦æ£€æµ‹åˆ°æ”»å‡»æ­¥éª¤çš„å‰ä¸€æ­¥ - ä½¿ç”¨æ–°çš„waitingæœºåˆ¶
        if not monitor.detected_pre_attack_step:
            # ä½¿ç”¨åŒæ­¥ç‰ˆæœ¬çš„æ£€æµ‹æ–¹æ³•
            if monitor.check_pre_attack_step(content, role, standardized_name):
                # è®°å½•å®é™…æ£€æµ‹åˆ°çš„å‰ä¸€æ­¥stepç¼–å·ï¼ˆå½“å‰æ­¥éª¤å°±æ˜¯å‰ä¸€æ­¥ï¼‰
                current_step = step_counter.get_step()
                monitor.record_pre_attack_detection(current_step)
                print(
                    f"[ADVANCED_ATTACK_MONITOR] æ£€æµ‹åˆ°æ”»å‡»æ­¥éª¤ {monitor.attack_step} çš„å‰ä¸€æ­¥! (å®é™…step: {current_step})"
                )
                print(f"[ADVANCED_ATTACK_MONITOR] å‡†å¤‡inä¸‹ä¸€æ­¥æ³¨å…¥æ”»å‡»prompt...")

                # æ–°å¢ï¼šè®¾ç½®waitingæ³¨å…¥çŠ¶æ€
                monitor.injection_pending = True
                monitor.injection_completed = False
                print(
                    f"[ADVANCED_ATTACK_MONITOR] è®¾ç½®waitingæ³¨å…¥çŠ¶æ€ï¼Œç³»ç»Ÿå°†waitingæ³¨å…¥å®Œæˆ..."
                )

        # å…ˆè®°å½•å½“å‰æ­¥éª¤
        current_step = step_counter.get_step()
        history.append(
            {
                "step": current_step,
                "content": content,
                "role": role,
                "name": standardized_name,
            }
        )
        # å¢åŠ æ­¥éª¤è®¡æ•°å™¨
        step_counter.increment()

        # åŒæ­¥æ‰€æœ‰Roleå®ä¾‹çš„æ­¥éª¤è®¡æ•°å™¨ï¼ˆä»…inåˆç†èŒƒå›´å†…ï¼‰
        current_step = step_counter.get_step()
        if current_step <= 1000:  # åªinåˆç†èŒƒå›´å†…åŒæ­¥
            for r in step_counter.roles.values():
                if hasattr(r, "step_counter"):
                    r.step_counter = current_step
        else:
            # å¦‚æœæ­¥éª¤è®¡æ•°å™¨å¼‚å¸¸ï¼Œé‡ç½®å®ƒ
            print(f"[DEBUG] æ£€æµ‹åˆ°æ­¥éª¤è®¡æ•°å™¨å¼‚å¸¸å€¼: {current_step}ï¼Œæ­£iné‡ç½®...")
            step_counter.reset()
            for r in step_counter.roles.values():
                if hasattr(r, "step_counter"):
                    r.step_counter = 0

        # åŒæ­¥ç›‘æ§å™¨çš„æ­¥éª¤è®¡æ•°å™¨
        monitor.update_step(step_counter.get_step())

    # æ„å»ºæ”»å‡»åçš„è§£å†³æ–¹æ¡ˆç”Ÿæˆæç¤º
    task_id = success_log["question_ID"]

    prompt = f"""Complete the following programming task, with a multi-agent team cooperation
    the solution file you write should be named as "{task_id.replace('/', '_')}_attacked_solution.py":

Task ID: {task_id}
Question: {success_log['question']}

"""

    try:
        # ä¿®æ”¹ï¼šåˆ›å»ºä¸€ä¸ªè‡ªå®šä¹‰çš„generate_repoå‡½æ•°æ¥è·å–Roleå®ä¾‹
        from metagpt.software_company import generate_repo as original_generate_repo
        from metagpt.team import Team
        from metagpt.context import Context
        from metagpt.config2 import config
        from metagpt.roles import (
            Architect,
            DataAnalyst,
            Engineer2,
            ProductManager,
            TeamLeader,
        )

        # åˆ›å»ºè‡ªå®šä¹‰çš„generate_repoå‡½æ•°
        def custom_generate_repo(idea, **kwargs):
            config.update_via_cli(
                kwargs.get("project_path", ""),
                kwargs.get("project_name", ""),
                kwargs.get("inc", False),
                kwargs.get("reqa_file", ""),
                kwargs.get("max_auto_summarize_code", 0),
            )
            ctx = Context(config=config)

            company = Team(context=ctx)
            roles = [
                TeamLeader(
                    project_dir=kwargs.get("project_dir"),
                    log_step=kwargs.get("log_step"),
                    attack_monitor=kwargs.get("attack_monitor"),
                ),
                ProductManager(
                    project_dir=kwargs.get("project_dir"),
                    log_step=kwargs.get("log_step"),
                    attack_monitor=kwargs.get("attack_monitor"),
                ),
                Architect(
                    project_dir=kwargs.get("project_dir"),
                    log_step=kwargs.get("log_step"),
                    attack_monitor=kwargs.get("attack_monitor"),
                ),
                Engineer2(
                    project_dir=kwargs.get("project_dir"),
                    log_step=kwargs.get("log_step"),
                    attack_monitor=kwargs.get("attack_monitor"),
                ),
                DataAnalyst(
                    project_dir=kwargs.get("project_dir"),
                    log_step=kwargs.get("log_step"),
                    attack_monitor=kwargs.get("attack_monitor"),
                ),
            ]

            company.hire(roles)

            # å°†Roleå®ä¾‹æ³¨å†Œåˆ°æ­¥éª¤è®¡æ•°å™¨
            for role in roles:
                step_counter.roles[role.name] = role
                # ç¡®ä¿attack_monitorä¹Ÿè¢«æ­£ç¡®è®¾ç½®
                if hasattr(role, "attack_monitor"):
                    role.attack_monitor = monitor

            company.invest(kwargs.get("investment", 3.0))
            return company.run(n_round=kwargs.get("n_round", 5), idea=idea)

        team_result = await custom_generate_repo(
            idea=prompt,
            n_round=5,
            code_review=True,
            run_tests=True,
            implement=True,
            project_name=f"attack_solution_{task_id.replace('/', '_')}",
            project_path=WORKSPACE_DIR,
            project_dir=WORKSPACE_DIR,
            log_step=log_step,
            use_async=True,
            attack_monitor=monitor,
        )

        # æ£€æŸ¥æ”»å‡»åçš„è§£å†³æ–¹æ¡ˆæ˜¯å¦ç”Ÿæˆ
        possible_paths = [
            os.path.join(
                WORKSPACE_DIR, f"{task_id.replace('/', '_')}_attacked_solution.py"
            ),
            os.path.join(
                WORKSPACE_DIR,
                f"attack_solution_{task_id.replace('/', '_')}",
                f"{task_id.replace('/', '_')}_attacked_solution.py",
            ),
            os.path.join(
                WORKSPACE_DIR,
                f"attack_solution_{task_id.replace('/', '_')}",
                "workspace",
                f"{task_id.replace('/', '_')}_attacked_solution.py",
            ),
        ]

        solution_file = None
        for path in possible_paths:
            if os.path.exists(path):
                solution_file = path
                break

        if solution_file:
            with open(solution_file, "r") as f:
                attacked_solution = f.read()

                # åˆ¤æ–­æ”»å‡»æ˜¯å¦Success
                monitor.attack_successful = (
                    attacked_solution != success_log["model_prediction"]
                )

                return attacked_solution, history, monitor
        else:
            print(f"Warning: Attacked solution file not found for {task_id}")
            print(f"Searched in paths: {possible_paths}")

            # å°è¯•LoadedHistoricalè®°å½•ä¸­æå–è§£å†³æ–¹æ¡ˆ
            solution = extract_solution_from_history(history)
            if solution:
                print(f"Extracted solution from history for {task_id}")
                monitor.attack_successful = solution != success_log["model_prediction"]
                return solution, history, monitor

            return None, None, monitor

    except Exception as e:
        print(f"Attacked solution generation failed: {e}")
        return None, None, monitor


def extract_solution_from_history(history):
    """LoadedHistoricalè®°å½•ä¸­æå–è§£å†³æ–¹æ¡ˆ"""
    for step in reversed(history):
        content = step.get("content", "")

        # æŸ¥æ‰¾Pythonä»£ç 
        if "def " in content or "import " in content:
            # æå–ä»£ç éƒ¨åˆ†
            lines = content.split("\n")
            code_lines = []
            in_code_block = False

            for line in lines:
                if "```python" in line or "```" in line:
                    in_code_block = not in_code_block
                    continue
                if in_code_block or line.strip().startswith(
                    ("def ", "import ", "class ", "from ")
                ):
                    code_lines.append(line)

            if code_lines:
                return "\n".join(code_lines)

    return None


async def process_success_log(success_log):
    """å¤„ç†å•ä¸ªSuccessçš„logæ–‡ä»¶ - è¢«universalæ¡†æ¶è°ƒç”¨"""
    log_name = f"{success_log['question_ID'].replace('/', '_')}.json"
    task_id = success_log["question_ID"]

    # æ–­ç‚¹ç»­è·‘ï¼šæ£€æŸ¥æ˜¯å¦å·²å®Œæˆæ‰€æœ‰æ­¥éª¤
    attacked_path = get_attacked_log_path(log_name)
    attack_suggestion_path = get_attack_suggestion_log_path(log_name)
    attack_record_path = get_attack_record_path(log_name)

    # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰å¿…éœ€çš„æ–‡ä»¶éƒ½å­˜in
    all_files_exist = (
        os.path.exists(attacked_path)
        and os.path.exists(attack_suggestion_path)
        and os.path.exists(attack_record_path)
    )

    # ä¿®æ”¹Skippedé€»è¾‘ï¼šåªæœ‰inRoundä¸€roundæ—¶æ‰Skippedï¼ŒRoundäºŒroundåº”è¯¥é‡æ–°ç”Ÿæˆæ”»å‡»åçš„è§£å†³æ–¹æ¡ˆ
    # inè¿è¡Œæ—¶åŠ¨æ€è·å–å½“å‰roundæ¬¡
    current_round = 3  # é»˜è®¤å€¼
    attack_suggestion_log_base = os.getenv(
        "ATTACK_SUGGESTION_LOG_BASE", ATTACK_SUGGESTION_LOG_BASE
    )

    if "round_2" in attack_suggestion_log_base:
        current_round = 2
    elif "round_1" in attack_suggestion_log_base:
        current_round = 1
    else:
        # å°è¯•Loadedè·¯å¾„ä¸­æå–roundæ¬¡
        import re

        match = re.search(r"round_(\d+)", attack_suggestion_log_base)
        if match:
            current_round = int(match.group(1))

    # åªæœ‰inRoundä¸€roundä¸”æ‰€æœ‰æ–‡ä»¶éƒ½å­˜inæ—¶æ‰Skipped
    if current_round == 1 and all_files_exist:
        print(
            f"[INFO] All logs for task {task_id} already exist in round 1, skipping..."
        )
        return

    # Failed to loadprevious attack analysis
    # ä¼˜å…ˆä½¿ç”¨æ¡†æ¶ä¼ é€’çš„Historicalåˆ†æä¿¡æ¯
    if "previous_attack_analyses" in success_log:
        previous_attack_analyses = success_log["previous_attack_analyses"]
        print(
            f"[FRAMEWORK_HISTORY] ä½¿ç”¨æ¡†æ¶ä¼ é€’çš„Historicalæ”»å‡»åˆ†æä¿¡æ¯: {len(previous_attack_analyses)} round"
        )
    else:
        # å‘åå…¼å®¹ï¼šä½¿ç”¨åŸæœ‰çš„Failed to loadæ–¹å¼
        previous_attack_analyses = load_previous_attack_analysis(task_id)
        print(f"[FALLBACK_HISTORY] ä½¿ç”¨åŸæœ‰æ–¹å¼Failed to loadHistoricalæ”»å‡»åˆ†æä¿¡æ¯")

    if previous_attack_analyses:
        print(
            f"[PREVIOUS_ATTACK] Loaded previous attack analyses for {task_id} from {len(previous_attack_analyses)} rounds."
        )
        for i, prev_analysis in enumerate(previous_attack_analyses):
            print(f"  Round {i+1}: step {prev_analysis.get('attack_step', 'Unknown')}")
    else:
        print(f"[PREVIOUS_ATTACK] No previous attack analyses found for {task_id}")

    # 1. æ”»å‡»åˆ†æ
    print(f"[ATTACK_ANALYSIS] Starting attack analysis for {task_id}")
    attack_analysis, attack_history = await attack_with_meta_company(
        success_log, 0, previous_attack_analyses
    )

    if attack_analysis is None:
        print(f"Attack analysis failed for {log_name}")
        return

    print(f"[ATTACK_ANALYSIS] Generated attack analysis for {task_id}")

    # ä¿å­˜æ”»å‡»åˆ†æç»“æœ
    with open(attack_suggestion_path, "w", encoding="utf-8") as f:
        json.dump(
            {
                "attack_analysis": attack_analysis,
                "history": attack_history,
                "previous_attack_analysis": previous_attack_analyses,
            },
            f,
            ensure_ascii=False,
            indent=2,
        )
    print(f"[ATTACK_ANALYSIS] Attack analysis completed for {task_id}")

    # 2. ç”Ÿæˆæ”»å‡»åçš„è§£å†³æ–¹æ¡ˆï¼ˆå¸¦é«˜çº§ç›‘æ§ï¼‰
    print(
        f"[ADVANCED_ATTACK_IMPLEMENTATION] Starting advanced attack implementation for {success_log['question_ID']}"
    )
    attacked_solution, attacked_history, monitor = (
        await generate_attacked_solution_with_advanced_monitoring(
            success_log, attack_analysis
        )
    )

    if attacked_solution is None:
        print(f"Attacked solution generation failed for {log_name}")
        # å³ä½¿failedä¹Ÿè¦ä¿å­˜ç›‘æ§è®°å½•
        with open(attack_record_path, "w", encoding="utf-8") as f:
            json.dump(monitor.get_attack_record(), f, ensure_ascii=False, indent=2)
        return

    # ä¿å­˜æ”»å‡»åçš„è§£å†³æ–¹æ¡ˆandæ—¥å¿—
    with open(attacked_path, "w", encoding="utf-8") as f:
        json.dump(
            {
                "question": success_log["question"],
                "question_ID": success_log["question_ID"],
                "ground_truth": success_log["ground_truth"],
                "model_prediction": attacked_solution,
                "history": attacked_history,
                "attack_analysis": attack_analysis,
            },
            f,
            ensure_ascii=False,
            indent=2,
        )

    # ä¿å­˜æ”»å‡»ç›‘æ§è®°å½•
    with open(attack_record_path, "w", encoding="utf-8") as f:
        json.dump(monitor.get_attack_record(), f, ensure_ascii=False, indent=2)

    print(
        f"[ADVANCED_ATTACK_RESULT] Task {success_log['question_ID']}: "
        f"Detected={monitor.detected_pre_attack_step}, "
        f"Injected={monitor.injected_prompt}, "
        f"Successful={monitor.attack_successful}"
    )

    # æ˜¾ç¤ºæ³¨å…¥Historical
    if monitor.injection_history:
        print(f"[INJECTION_HISTORY] æ³¨å…¥Historical:")
        for i, injection in enumerate(monitor.injection_history):
            print(f"  {i+1}. ç­–ç•¥: {injection['strategy']}, æ­¥éª¤: {injection['step']}")
